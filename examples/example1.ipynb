{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create NaiveDense Model\n",
    "\n",
    "The `NaiveDense` class is a simple implementation of a dense (fully connected) neural network layer. It allows you to create a basic dense layer with specified input and output sizes, along with an activation function of your choice. This implementation includes the following functionalities:\n",
    "\n",
    "- **Initialization:** The class constructor (`__init__`) takes `input_size`, `output_size`, and `activation` as parameters. It initializes the layer's weights and bias. The weights (`W`) are randomly initialized using uniform values, and the bias (`b`) is initialized to zeros.\n",
    "\n",
    "- **Forward Pass:** The `__call__` method performs the forward pass of the layer. It takes `inputs` as input and computes the output of the layer by applying the activation function to the matrix multiplication of the inputs and the weights, followed by the addition of the bias.\n",
    "\n",
    "- **Weight Access:** The `weights` property provides a convenient way to access the layer's weights and bias as a list, which is useful for weight initialization, visualization, or other purposes.\n",
    "\n",
    "This `NaiveDense` class offers a basic building block for constructing neural network architectures and is suitable for educational purposes or for implementing simple models. Keep in mind that this implementation lacks some features and optimizations found in production-level deep learning frameworks.\n",
    "\n",
    "To use the `NaiveDense` class, you can create an instance with the desired input size, output size, and activation function. Then, you can call the instance on input data to get the layer's output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-21 19:50:31.966943: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-08-21 19:50:32.010069: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-08-21 19:50:32.010585: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-08-21 19:50:32.789154: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf;\n",
    "\n",
    "class NaiveDense :\n",
    "    def __init__(self, input_size, output_size, activation): \n",
    "        self.activation = activation\n",
    "        w_shape = (input_size, output_size)\n",
    "        w_initial_value = tf.random.uniform(w_shape, minval=0, maxval=1e-1)\n",
    "        self.W = tf.Variable(w_initial_value)\n",
    "        b_shape = (output_size, )\n",
    "        b_initial_value = tf.zeros(b_shape)\n",
    "        self.b = tf.Variable(b_initial_value)\n",
    "\n",
    "    def __call__(self, inputs):\n",
    "        return self.activation(tf.matmul(inputs, self.W) + self.b)\n",
    "    \n",
    "    @property\n",
    "    def weights(self):\n",
    "        return [self.W, self.b]\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create NaiveSequential Model\n",
    "\n",
    "The `NaiveSequential` class represents a simple sequential neural network model. It is designed to stack multiple layers sequentially to create a neural network architecture. This class offers the following features:\n",
    "\n",
    "- **Initialization:** The constructor (`__init__`) takes a list of `layers` as input. These layers are added sequentially to the model.\n",
    "\n",
    "- **Forward Pass:** The `__call__` method performs a forward pass through the model. It takes `inputs` as input and applies each layer in the sequence to the inputs, building the network's output step by step.\n",
    "\n",
    "- **Weight Access:** The `weights` property provides a way to access the weights and biases of all layers in the model. It aggregates the weights of each layer and returns them as a list.\n",
    "\n",
    "The `NaiveSequential` class simplifies the process of creating and using a basic neural network model by allowing you to define a sequential architecture and perform forward passes with ease. While this class lacks some advanced functionalities found in more sophisticated deep learning libraries, it serves as a useful starting point for educational purposes or simple model building."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NaiveSequential:\n",
    "    def __init__(self, layers):\n",
    "        self.layers = layers\n",
    "\n",
    "    def __call__(self, inputs):\n",
    "        x = inputs\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "\n",
    "    @property\n",
    "    def weights(self):\n",
    "        weights = []\n",
    "        for layer in self.layers:\n",
    "            weights += layer.weights\n",
    "        return weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create NaiveSequential Model with Two NaiveDense Layers\n",
    "\n",
    "The provided code snippet demonstrates the creation of a neural network model using the `NaiveSequential` class. This model is comprised of two `NaiveDense` layers, each with distinct input and output sizes, as well as activation functions. The layers are organized sequentially to establish the architecture of the neural network.\n",
    "\n",
    "The model's construction can be summarized as follows:\n",
    "\n",
    "1. **First Layer (`NaiveDense`):**\n",
    "   - Input size: 28 x 28 (784 pixels)\n",
    "   - Output size: 512\n",
    "   - Activation function: ReLU (Rectified Linear Unit)\n",
    "\n",
    "2. **Second Layer (`NaiveDense`):**\n",
    "   - Input size: 512\n",
    "   - Output size: 10\n",
    "   - Activation function: Softmax\n",
    "\n",
    "This architecture is designed to transform input data, perform intermediate computations, and generate outputs suitable for classification tasks. By using the `NaiveSequential` class, you can conveniently construct and configure neural network models with sequential layers. The `weights` property allows you to access the weights associated with the layers in the model for analysis and evaluation purposes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NaiveSequential([\n",
    "    NaiveDense(input_size=28 * 28, output_size=512, activation=tf.nn.relu),\n",
    "    NaiveDense(input_size=512, output_size=10, activation=tf.nn.softmax)\n",
    "])\n",
    "\n",
    "assert len(model.weights) == 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a Batch Generator\n",
    "\n",
    "The `BatchGenerator` class facilitates the creation of batches from a given dataset for efficient training in machine learning models. This class streamlines the process of loading and organizing data during training, promoting better resource utilization and quicker convergence. Here's an overview of the `BatchGenerator` class:\n",
    "\n",
    "- **Initialization (`__init__`):**\n",
    "  The constructor takes `images` and `labels` as input, representing the dataset. It also includes an optional parameter, `batch_size`, to determine the size of each batch. The constructor asserts that the length of the input images matches the length of the labels.\n",
    "\n",
    "- **Batch Retrieval (`next`):**\n",
    "  The `next` method retrieves the next batch of images and labels from the dataset. It uses the `index` attribute to keep track of the current position in the dataset. The batch is extracted by slicing the dataset from `index` to `index + batch_size`. After retrieval, the `index` is updated to point to the next batch.\n",
    "\n",
    "- **Batch Generation (`__call__`):**\n",
    "  The `__call__` method is provided for a more convenient way to generate batches. It simply calls the `next` method to retrieve and return the next batch.\n",
    "\n",
    "By employing the `BatchGenerator` class, data can be efficiently loaded in batches, reducing memory requirements and allowing models to process data more swiftly. This class is particularly useful when working with large datasets that cannot be loaded entirely into memory. It ensures a steady and optimized flow of data for model training and validation, contributing to enhanced training performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "class BatchGenerator : \n",
    "    def __init__(self, images, labels, batch_size=128):\n",
    "        assert len(images) == len(labels)\n",
    "        self.index = 0\n",
    "        self.images = images\n",
    "        self.labels = labels\n",
    "        self.batch_size = batch_size\n",
    "        self.num_batches = math.ceil(len(images) / batch_size)\n",
    "\n",
    "    def next(self):\n",
    "        images = self.images[self.index : self.index + self.batch_size]\n",
    "        labels = self.labels[self.index : self.index + self.batch_size]\n",
    "        self.index += self.batch_size\n",
    "        return images, labels\n",
    "\n",
    "    def __call__(self):\n",
    "        return self.next()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One Training Step with Gradient Descent\n",
    "The provided code snippet illustrates the implementation of a single training step utilizing the gradient descent optimization technique. This process is a pivotal element in training neural network models to enhance their predictive capabilities. Here's a breakdown of the code:\n",
    "\n",
    "### `one_training_step` Function\n",
    "- **Forward Pass and Loss Calculation:**\n",
    "  - The function takes the neural network `model`, `images_batch`, and `labels_batch` as inputs.\n",
    "  - Within a `tf.GradientTape` context, it computes predictions by passing the `images_batch` through the model.\n",
    "  - Per-sample losses are computed using the sparse categorical cross-entropy loss function.\n",
    "  - The average loss is derived by calculating the mean of the per-sample losses.\n",
    "  \n",
    "- **Gradient Computation and Weight Update:**\n",
    "  - Gradients of the average loss with respect to the model's weights are determined using the `tape.gradient` method.\n",
    "  - The `update_weights` function is called to update the model's weights using the computed gradients and a predefined learning rate.\n",
    "  - The learning rate (`learning_rate`) determines the magnitude of weight adjustments.\n",
    "  \n",
    "- **Return Value:**\n",
    "  - The function returns the calculated average loss.\n",
    "\n",
    "### `update_weights` Function and Learning Rate\n",
    "- The `update_weights` function iterates through gradients and weights, applying weight updates based on the gradients and the learning rate.\n",
    "- The learning rate (`learning_rate`) determines the step size for weight adjustments in the gradient descent process.\n",
    "\n",
    "This iterative process seeks to minimize the loss and improve the model's accuracy and performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_training_step(model, images_batch, labels_batch):\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions = model(images_batch)\n",
    "        per_sample_losses = tf.keras.losses.sparse_categorical_crossentropy(labels_batch, predictions)\n",
    "        average_loss = tf.reduce_mean(per_sample_losses)\n",
    "    gradients = tape.gradient(average_loss, model.weights)\n",
    "    update_weights(gradients, model.weights)\n",
    "    return average_loss\n",
    "\n",
    "learning_rate = 1e-3\n",
    "\n",
    "def update_weights(gradients, weights):\n",
    "    for g, w in zip(gradients, model.weights):\n",
    "        w.assign_sub(g * learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Full Training Loop\n",
    "\n",
    "The code snippet provides an overview of the complete training loop for training a neural network model. This loop encompasses multiple epochs and batch-wise training steps. The key components of the training loop are as follows:\n",
    "\n",
    "### `fit` Function: Full Training Loop\n",
    "\n",
    "- **Function Description:**\n",
    "  The `fit` function performs the entire training process for a neural network model.\n",
    "\n",
    "- **Input Arguments:**\n",
    "  - `model`: The neural network model to be trained.\n",
    "  - `images`: The training images dataset.\n",
    "  - `labels`: The corresponding labels for the training images.\n",
    "  - `epochs`: The number of training epochs.\n",
    "  - `batch_size`: The size of each training batch (default is 128).\n",
    "\n",
    "- **Training Loop:**\n",
    "  The outer loop iterates over the specified number of epochs (`epochs`).\n",
    "  - For each epoch, the current epoch counter is displayed.\n",
    "\n",
    "- **Batch Generation and Training Steps:**\n",
    "  - A `BatchGenerator` is initialized with the training images, labels, and the specified batch size.\n",
    "  - The inner loop iterates over the batches within the current epoch.\n",
    "  - For each batch, the `images_batch` and `labels_batch` are extracted from the `batch_generator`.\n",
    "  - A training step is performed using the `one_training_step` function, updating the model's weights and calculating the loss.\n",
    "\n",
    "- **Loss Logging:**\n",
    "  - After each training step (batch), if the batch counter is a multiple of 100, the current loss is displayed for monitoring the training progress.\n",
    "\n",
    "This loop facilitates the iterative improvement of the model's performance by adjusting its weights based on computed gradients and minimizing the loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(model, images, labels, epochs, batch_size=128):\n",
    "    for epoch_counter in range(epochs):\n",
    "        print('Epoch %d' % epoch_counter)\n",
    "        batch_generator = BatchGenerator(images, labels)\n",
    "        for batch_counter in range(batch_generator.num_batches):\n",
    "            images_batch, labels_batch = batch_generator.next()\n",
    "            loss = one_training_step(model, images_batch, labels_batch)\n",
    "            if batch_counter % 100 == 0:\n",
    "                print('loss at batch %d: %.2f' % (batch_counter, loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation and Training Initialization\n",
    "\n",
    "The provided code snippet outlines the process of preparing data and initializing the training for a neural network model utilizing Keras. Here's an overview of the steps performed:\n",
    "\n",
    "### Data Preparation\n",
    "\n",
    "- **Data Loading:** The MNIST dataset is loaded using the `mnist.load_data()` function, resulting in training and testing sets of images and labels.\n",
    "\n",
    "- **Data Reshaping:** The images are reshaped into a flattened format `(num_samples, 28 * 28)` to match the model's input requirements.\n",
    "\n",
    "- **Data Normalization:** The pixel values of the images are normalized to a range between 0 and 1 by dividing by 255, which is a common practice in data preprocessing.\n",
    "\n",
    "### Training Initialization\n",
    "\n",
    "- **Model Training:** The `fit` function is used to initiate the training process for the neural network model. It takes the model, training images, training labels, number of epochs, and optionally the batch size as inputs.\n",
    "\n",
    "By following these steps, the code sets the stage for training a neural network model on image classification tasks using the MNIST dataset. The preparation of data and training initialization are crucial components in the overall training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0\n",
      "loss at batch 0: 4.04\n",
      "loss at batch 100: 2.20\n",
      "loss at batch 200: 2.19\n",
      "loss at batch 300: 2.06\n",
      "loss at batch 400: 2.19\n",
      "Epoch 1\n",
      "loss at batch 0: 1.88\n",
      "loss at batch 100: 1.85\n",
      "loss at batch 200: 1.82\n",
      "loss at batch 300: 1.69\n",
      "loss at batch 400: 1.81\n",
      "Epoch 2\n",
      "loss at batch 0: 1.56\n",
      "loss at batch 100: 1.56\n",
      "loss at batch 200: 1.50\n",
      "loss at batch 300: 1.41\n",
      "loss at batch 400: 1.50\n",
      "Epoch 3\n",
      "loss at batch 0: 1.31\n",
      "loss at batch 100: 1.32\n",
      "loss at batch 200: 1.23\n",
      "loss at batch 300: 1.20\n",
      "loss at batch 400: 1.27\n",
      "Epoch 4\n",
      "loss at batch 0: 1.11\n",
      "loss at batch 100: 1.14\n",
      "loss at batch 200: 1.04\n",
      "loss at batch 300: 1.04\n",
      "loss at batch 400: 1.11\n",
      "Epoch 5\n",
      "loss at batch 0: 0.97\n",
      "loss at batch 100: 1.01\n",
      "loss at batch 200: 0.90\n",
      "loss at batch 300: 0.92\n",
      "loss at batch 400: 0.99\n",
      "Epoch 6\n",
      "loss at batch 0: 0.86\n",
      "loss at batch 100: 0.90\n",
      "loss at batch 200: 0.79\n",
      "loss at batch 300: 0.83\n",
      "loss at batch 400: 0.90\n",
      "Epoch 7\n",
      "loss at batch 0: 0.78\n",
      "loss at batch 100: 0.82\n",
      "loss at batch 200: 0.71\n",
      "loss at batch 300: 0.76\n",
      "loss at batch 400: 0.83\n",
      "Epoch 8\n",
      "loss at batch 0: 0.72\n",
      "loss at batch 100: 0.75\n",
      "loss at batch 200: 0.65\n",
      "loss at batch 300: 0.71\n",
      "loss at batch 400: 0.78\n",
      "Epoch 9\n",
      "loss at batch 0: 0.67\n",
      "loss at batch 100: 0.70\n",
      "loss at batch 200: 0.60\n",
      "loss at batch 300: 0.67\n",
      "loss at batch 400: 0.74\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.datasets import mnist\n",
    "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
    "\n",
    "train_images = train_images.reshape((60000, 28 * 28))\n",
    "\n",
    "train_images = train_images.astype('float32') / 255\n",
    "\n",
    "test_images = test_images.reshape((10000, 28 * 28))\n",
    "\n",
    "test_images = test_images.astype('float32') / 255\n",
    "\n",
    "fit(model, train_images, train_labels, epochs=10, batch_size=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating the Model\n",
    "\n",
    "The provided code snippet focuses on evaluating the performance of a trained neural network model using a test dataset. The key steps involved in this evaluation process are as follows:\n",
    "\n",
    "### Prediction and Evaluation\n",
    "\n",
    "- **Prediction:** The trained `model` is utilized to generate predictions for the `test_images` dataset. This results in a `predictions` array, computed using NumPy.\n",
    "\n",
    "- **Predicted Labels:** Using the `argmax` function along the appropriate axis, the `predicted_labels` are derived from the `predictions`. These labels represent the classes predicted by the model for each input image.\n",
    "\n",
    "- **Accuracy Calculation:** By comparing the `predicted_labels` with the actual `test_labels`, the code calculates the accuracy of the model's predictions. This is achieved by creating the `matches` array, which contains Boolean values indicating whether the predictions match the ground truth labels.\n",
    "\n",
    "### Displaying Results\n",
    "\n",
    "- **Accuracy Display:** The accuracy of the model's predictions is computed by calculating the mean of the `matches` array. This value reflects the proportion of correct predictions made by the model on the test dataset.\n",
    "\n",
    "The code's objective is to quantitatively assess the model's effectiveness in classifying new, unseen data. The accuracy metric provides valuable insights into the model's performance and its ability to generalize well to new examples. This evaluation step is vital for understanding the real-world applicability and robustness of the trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.81\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "predictions = model(test_images)\n",
    "predictions  = predictions.numpy()\n",
    "\n",
    "predicted_labels = np.argmax(predictions, axis=1)\n",
    "\n",
    "matches = predicted_labels == test_labels\n",
    "\n",
    "print('accuracy: %.2f' % matches.mean())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
